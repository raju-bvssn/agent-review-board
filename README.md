# üöÄ **Agent Review Board (ARB)**

A multi-agent system for iterative content refinement with mandatory human-in-the-loop oversight.

---

## **Overview**

Agent Review Board is a collaborative AI system where multiple specialized reviewer agents provide feedback on content generated by a presenter agent. The system ensures quality through:

* **Multi-agent collaboration** ‚Äî Presenter and specialized reviewer agents
* **Mandatory HITL** ‚Äî Human approval required at each iteration
* **Incognito mode** ‚Äî No persistent storage, privacy-first design
* **Flexible LLM support** ‚Äî Works with multiple LLM providers

---

## **Features**

‚úÖ **Iterative Refinement** ‚Äî Content improves through multiple review cycles

‚úÖ **Specialized Reviewers** ‚Äî Choose from various expert roles (2-3 per session)

‚úÖ **Human-in-the-Loop** ‚Äî Approve or modify feedback before next iteration

‚úÖ **Confidence Scoring** ‚Äî Automated quality and convergence evaluation

‚úÖ **Incognito Mode** ‚Äî No data persistence, session cleared on exit

‚úÖ **Multi-Provider Support** ‚Äî OpenAI, Anthropic, Gemini, HuggingFace, Ollama (FREE options available!)

---

## **Quick Start**

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure LLM Provider

The Agent Review Board supports multiple LLM providers, including **FREE options** perfect for demos!

#### **Option A: üÜì Google Gemini (FREE Tier)**

1. Get a FREE API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
2. In the app, navigate to "LLM Settings"
3. Select "Google Gemini"
4. Enter your `GEMINI_API_KEY`
5. Select `gemini-1.5-flash` model (FREE, 15 req/min)

```bash
# Or set environment variable:
export GEMINI_API_KEY="your-key-here"
```

#### **Option B: üÜì HuggingFace Inference API (FREE)**

1. Get a FREE token from [HuggingFace Settings](https://huggingface.co/settings/tokens)
2. In the app, select "HuggingFace"
3. Enter your `HUGGINGFACE_API_KEY`
4. Use FREE models like `tiiuae/falcon-7b-instruct`

```bash
# Or set environment variable:
export HUGGINGFACE_API_KEY="your-token-here"
```

#### **Option C: üÜì Ollama (Local - Completely FREE)**

1. Download Ollama from [https://ollama.com/download](https://ollama.com/download)
2. Install and start Ollama
3. Pull free models:

```bash
ollama pull llama3
ollama pull mistral
ollama pull phi3
ollama pull codellama
ollama pull qwen2.5
```

4. In the app, select "Ollama (Local)" - No API key required!
5. Models run locally on your machine

#### **Option D: OpenAI (Paid)**

1. Get API key from [OpenAI Platform](https://platform.openai.com/api-keys)
2. Select "OpenAI" in LLM Settings
3. Enter your `OPENAI_API_KEY`

```bash
export OPENAI_API_KEY="your-key-here"
```

#### **Option E: Anthropic (Paid)**

1. Get API key from [Anthropic Console](https://console.anthropic.com/)
2. Select "Anthropic" in LLM Settings
3. Enter your `ANTHROPIC_API_KEY`

```bash
export ANTHROPIC_API_KEY="your-key-here"
```

#### **Option F: Mock Provider (Testing Only)**

No setup required - select "Mock Provider" for testing the UI without any API calls.

### 3. Run the Application

```bash
streamlit run streamlit_app.py
```

Or for custom port:

```bash
streamlit run streamlit_app.py --server.port 8504
```

### 4. Start a Session

* Click "Start Session"
* Enter requirements/description
* Upload reference files (optional)
* Select 2-3 reviewer roles
* Choose models for each agent
* Click "Start Session"

---

## **üÜì FREE Demo Setup (Recommended)**

For a completely free demo without any API keys:

### **Method 1: Ollama (Local, Private, Fast)**

```bash
# 1. Install Ollama
brew install ollama  # macOS
# or download from https://ollama.com/download

# 2. Pull a model
ollama pull llama3

# 3. Start Ollama (runs automatically on macOS/Linux)
ollama serve

# 4. Run the app
streamlit run streamlit_app.py

# 5. In LLM Settings, select "Ollama (Local)"
```

### **Method 2: Google Gemini (Cloud, FREE Tier)**

```bash
# 1. Get FREE API key from https://makersuite.google.com/app/apikey

# 2. Set environment variable
export GEMINI_API_KEY="your-free-key"

# 3. Run the app
streamlit run streamlit_app.py

# 4. In LLM Settings, select "Google Gemini" and use "gemini-1.5-flash"
```

---

## **Architecture**

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed system design.

### Folder Structure

```
agent-review-board/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ ui/              # Streamlit UI components
‚îÇ   ‚îú‚îÄ‚îÄ core/            # Business logic & orchestration
‚îÇ   ‚îú‚îÄ‚îÄ agents/          # Agent implementations
‚îÇ   ‚îú‚îÄ‚îÄ llm/             # LLM provider abstraction
‚îÇ   ‚îú‚îÄ‚îÄ models/          # Pydantic data models
‚îÇ   ‚îî‚îÄ‚îÄ utils/           # Utility functions
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ unit/            # Unit tests
‚îÇ   ‚îî‚îÄ‚îÄ integration/     # Integration tests
‚îú‚îÄ‚îÄ streamlit_app.py     # Application entrypoint
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ RULES.md            # Development rules
‚îî‚îÄ‚îÄ ARCHITECTURE.md     # System architecture
```

---

## **Development Rules**

See [RULES.md](RULES.md) for complete development guidelines.

### Key Principles

* ‚ö†Ô∏è **No Streamlit in core logic** ‚Äî Keep UI separate from business logic
* ‚ö†Ô∏è **HITL is mandatory** ‚Äî Never auto-advance iterations
* ‚ö†Ô∏è **No persistence** ‚Äî All data in memory only
* ‚ö†Ô∏è **Type hints everywhere** ‚Äî Maintain strong typing
* ‚ö†Ô∏è **Test coverage required** ‚Äî All components must have tests

---

## **Testing**

### Run All Tests

```bash
pytest
```

### Run Specific Test Suite

```bash
pytest tests/unit/
pytest tests/integration/
```

### Run with Coverage

```bash
pytest --cov=app tests/
```

---

## **Security & Privacy**

### üîí Incognito Mode

* **No persistent storage** ‚Äî Session data lives in memory only
* **No API key storage** ‚Äî Keys cleared on session end
* **No file retention** ‚Äî Uploaded files deleted after session
* **Browser refresh clears everything** ‚Äî Complete privacy

### üõ°Ô∏è API Key Safety

* Never stored to disk
* Never logged
* Masked in UI
* Cleared from memory on exit

---

## **LLM Provider Comparison**

| Provider | Cost | Setup | Speed | Privacy | Best For |
|----------|------|-------|-------|---------|----------|
| **Ollama** | üÜì FREE | Medium | Fast | ‚úÖ Local | Development, demos, privacy |
| **Gemini** | üÜì FREE* | Easy | Fast | ‚òÅÔ∏è Cloud | Quick demos, testing |
| **HuggingFace** | üÜì FREE* | Easy | Slow | ‚òÅÔ∏è Cloud | Experimentation |
| **OpenAI** | üí∞ Paid | Easy | Fast | ‚òÅÔ∏è Cloud | Production, best quality |
| **Anthropic** | üí∞ Paid | Easy | Fast | ‚òÅÔ∏è Cloud | Production, best quality |
| **Mock** | üÜì FREE | None | Instant | ‚úÖ Local | Testing only |

*Free tier with limits

---

## **Extending the System**

### Add a New LLM Provider

1. Create new provider class in `app/llm/`
2. Inherit from `BaseLLMProvider`
3. Implement required methods:
   - `generate_text(prompt) -> str`
   - `list_models() -> List[str]`
   - `validate_connection() -> bool`
   - `get_provider_name() -> str`
4. Register in `ProviderFactory.PROVIDERS`
5. Add UI configuration in `app/ui/pages/llm_settings.py`
6. Add tests in `tests/test_provider_*.py`

### Add a New Reviewer Role

1. Subclass `ReviewerAgent` in `app/agents/reviewer.py`
2. Define specialized prompt template and focus areas
3. Register in role configuration
4. Add to UI dropdown in start session page

---

## **Project Status**

**Phase 1:** ‚úÖ Complete - Scaffolding & Architecture

* Folder structure
* Architecture documentation
* UI skeleton
* Stub implementations
* Test scaffolding

**Phase 2:** ‚úÖ Complete - Intelligence Layer

* Production LLM providers (OpenAI, Anthropic, Gemini, HuggingFace, Ollama)
* Real agent logic (Presenter, Reviewers, Confidence)
* Orchestrator with HITL enforcement
* Full test coverage
* FREE provider options for demos

**Phase 3:** üéØ Current - Enhanced Features

* Multi-provider support with free tiers
* Improved UI/UX
* Additional agent roles
* Performance optimizations

---

## **License**

MIT License - See LICENSE file for details

---

## **Contributing**

1. Read [RULES.md](RULES.md) and [ARCHITECTURE.md](ARCHITECTURE.md)
2. Follow strict separation of concerns
3. Write tests for all new code
4. Use type hints and docstrings
5. Submit PR with clear description

---

**Built with ‚ù§Ô∏è for safe, human-supervised AI collaboration**

