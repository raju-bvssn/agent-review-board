"""Orchestrator for managing agent iteration cycles."""

from typing import List, Dict, Optional, Any
from app.core.session_manager import SessionManager
from app.agents.presenter import PresenterAgent
from app.agents.reviewer import (
    ReviewerAgent, TechnicalReviewer, ClarityReviewer, 
    SecurityReviewer, BusinessReviewer, UXReviewer
)
from app.agents.confidence import ConfidenceAgent
from app.llm.base_provider import BaseLLMProvider
from app.models.feedback import Feedback


class IterationResult:
    """Result of a single iteration cycle."""
    
    def __init__(
        self,
        iteration: int,
        presenter_output: str,
        reviewer_feedback: List[Feedback],
        confidence_result: Dict[str, Any],
        error: Optional[str] = None
    ):
        """Initialize iteration result.
        
        Args:
            iteration: Iteration number
            presenter_output: Content generated by presenter
            reviewer_feedback: List of feedback from all reviewers
            confidence_result: Confidence evaluation result
            error: Error message if cycle failed
        """
        self.iteration = iteration
        self.presenter_output = presenter_output
        self.reviewer_feedback = reviewer_feedback
        self.confidence_result = confidence_result
        self.error = error
        self.human_gate_approved = False


class Orchestrator:
    """Orchestrates the agent review cycle.
    
    Responsibilities:
    - Run presenter → reviewers → confidence cycle
    - Enforce HITL gates
    - Handle errors gracefully
    - Store iteration history
    """
    
    # Map role names to reviewer classes
    REVIEWER_CLASSES = {
        'Technical Reviewer': TechnicalReviewer,
        'Clarity Reviewer': ClarityReviewer,
        'Security Reviewer': SecurityReviewer,
        'Business Reviewer': BusinessReviewer,
        'UX Reviewer': UXReviewer,
    }
    
    def __init__(
        self,
        session_manager: SessionManager,
        llm_provider: BaseLLMProvider
    ):
        """Initialize orchestrator.
        
        Args:
            session_manager: Session manager instance
            llm_provider: LLM provider for all agents
        """
        self.session_manager = session_manager
        self.llm_provider = llm_provider
        self.iteration_history: List[IterationResult] = []
        self.current_iteration_result: Optional[IterationResult] = None
    
    def run_iteration(
        self,
        requirements: str,
        selected_roles: List[str],
        file_summaries: Optional[List[str]] = None,
        approved_feedback: Optional[List[str]] = None
    ) -> IterationResult:
        """Run a complete iteration cycle.
        
        Args:
            requirements: User requirements
            selected_roles: List of reviewer roles
            file_summaries: Optional file summaries
            approved_feedback: Optional approved feedback from previous iteration
            
        Returns:
            IterationResult object
        """
        session = self.session_manager.get_current_session()
        if not session:
            raise ValueError("No active session")
        
        iteration = session.iteration
        
        try:
            # Step 1: Run Presenter
            presenter_output = self._run_presenter(
                requirements,
                approved_feedback,
                file_summaries
            )
            
            # Step 2: Run Reviewers
            reviewer_feedback = self._run_reviewers(
                presenter_output,
                selected_roles,
                iteration
            )
            
            # Step 3: Run Confidence Agent
            confidence_result = self._run_confidence(
                presenter_output,
                reviewer_feedback
            )
            
            # Create result
            result = IterationResult(
                iteration=iteration,
                presenter_output=presenter_output,
                reviewer_feedback=reviewer_feedback,
                confidence_result=confidence_result,
                error=None
            )
            
            # Store result
            self.current_iteration_result = result
            self.iteration_history.append(result)
            
            return result
        
        except Exception as e:
            # Handle errors gracefully
            error_msg = f"Iteration {iteration} failed: {str(e)}"
            
            result = IterationResult(
                iteration=iteration,
                presenter_output="",
                reviewer_feedback=[],
                confidence_result={"score": 0, "reasoning": "Iteration failed"},
                error=error_msg
            )
            
            self.current_iteration_result = result
            self.iteration_history.append(result)
            
            return result
    
    def _run_presenter(
        self,
        requirements: str,
        approved_feedback: Optional[List[str]],
        file_summaries: Optional[List[str]]
    ) -> str:
        """Run the presenter agent.
        
        Args:
            requirements: User requirements
            approved_feedback: Optional approved feedback
            file_summaries: Optional file summaries
            
        Returns:
            Presenter output string
        """
        presenter = PresenterAgent(self.llm_provider)
        
        # Get previous output if this is a refinement
        previous_output = None
        if len(self.iteration_history) > 0:
            previous_output = self.iteration_history[-1].presenter_output
        
        output = presenter.generate(
            requirements=requirements,
            feedback=approved_feedback,
            previous_output=previous_output,
            file_summaries=file_summaries
        )
        
        return output
    
    def _run_reviewers(
        self,
        content: str,
        selected_roles: List[str],
        iteration: int
    ) -> List[Feedback]:
        """Run all reviewer agents.
        
        Args:
            content: Content to review
            selected_roles: List of reviewer roles
            iteration: Current iteration number
            
        Returns:
            List of Feedback objects
        """
        feedback_list = []
        
        # Get previous feedback for iterative review
        previous_feedback_by_role = {}
        if len(self.iteration_history) > 0:
            last_result = self.iteration_history[-1]
            for feedback in last_result.reviewer_feedback:
                # Convert feedback points to readable string
                feedback_text = "\n".join([f"- {point}" for point in feedback.feedback_points])
                previous_feedback_by_role[feedback.reviewer_role] = feedback_text
        
        for role in selected_roles:
            try:
                # Get reviewer class
                reviewer_class = self.REVIEWER_CLASSES.get(role, ReviewerAgent)
                
                # Create reviewer instance
                reviewer = reviewer_class(self.llm_provider)
                
                # Get previous feedback for this role
                previous_feedback = previous_feedback_by_role.get(role, None)
                
                # Get feedback (with previous context if available)
                feedback = reviewer.review(content, iteration, previous_feedback=previous_feedback)
                feedback_list.append(feedback)
            
            except Exception as e:
                # Create error feedback
                error_feedback = Feedback(
                    reviewer_role=role,
                    feedback_points=[f"Review failed: {str(e)}"],
                    iteration=iteration,
                    approved=False,
                    modified=False
                )
                feedback_list.append(error_feedback)
        
        return feedback_list
    
    def _run_confidence(
        self,
        content: str,
        feedback_list: List[Feedback]
    ) -> Dict[str, Any]:
        """Run the confidence agent.
        
        Args:
            content: Presenter content
            feedback_list: List of reviewer feedback
            
        Returns:
            Confidence result dictionary
        """
        confidence_agent = ConfidenceAgent(self.llm_provider)
        
        result = confidence_agent.score(content, feedback_list)
        
        return result
    
    def approve_current_iteration(self, modified_feedback: Optional[Dict[str, List[str]]] = None):
        """Approve the current iteration and prepare for next.
        
        Args:
            modified_feedback: Optional dict of modified feedback by reviewer role
        """
        if not self.current_iteration_result:
            raise ValueError("No current iteration to approve")
        
        # Mark as approved
        self.current_iteration_result.human_gate_approved = True
        
        # Apply modifications if provided
        if modified_feedback:
            for feedback in self.current_iteration_result.reviewer_feedback:
                if feedback.reviewer_role in modified_feedback:
                    feedback.feedback_points = modified_feedback[feedback.reviewer_role]
                    feedback.modified = True
                feedback.approved = True
        else:
            # Approve all as-is
            for feedback in self.current_iteration_result.reviewer_feedback:
                feedback.approved = True
        
        # Increment iteration counter
        self.session_manager.increment_iteration()
    
    def reject_current_iteration(self):
        """Reject the current iteration.
        
        This marks the iteration as rejected and allows regeneration.
        """
        if not self.current_iteration_result:
            raise ValueError("No current iteration to reject")
        
        self.current_iteration_result.human_gate_approved = False
    
    def can_proceed_to_next_iteration(self) -> bool:
        """Check if we can proceed to the next iteration.
        
        Returns:
            True if HITL gate is passed, False otherwise
        """
        if not self.current_iteration_result:
            return False
        
        return self.current_iteration_result.human_gate_approved
    
    def get_approved_feedback(self) -> List[str]:
        """Get approved feedback from current iteration for next iteration.
        
        Returns:
            List of approved feedback strings
        """
        if not self.current_iteration_result:
            return []
        
        approved = []
        for feedback in self.current_iteration_result.reviewer_feedback:
            if feedback.approved:
                approved.extend(feedback.feedback_points)
        
        return approved
    
    def get_iteration_history(self) -> List[IterationResult]:
        """Get complete iteration history.
        
        Returns:
            List of IterationResult objects
        """
        return self.iteration_history.copy()
    
    def get_current_result(self) -> Optional[IterationResult]:
        """Get current iteration result.
        
        Returns:
            Current IterationResult or None
        """
        return self.current_iteration_result
    
    def reset(self):
        """Reset orchestrator state."""
        self.iteration_history.clear()
        self.current_iteration_result = None

